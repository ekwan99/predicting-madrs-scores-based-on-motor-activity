{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "repo_path = os.getcwd()\n",
    "folder_path = os.path.join(repo_path, \"input\")\n",
    "\n",
    "for dirname, _, filenames in os.walk(folder_path):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in MADRS1 and MADRS2 scores\n",
    "scores_path = os.path.join(folder_path, \"scores.csv\")\n",
    "scores = pd.read_csv(scores_path)\n",
    "MADRS1 = np.array(scores[\"madrs1\"])\n",
    "MADRS2 = np.array(scores[\"madrs2\"])\n",
    "MADRS = np.vstack((MADRS1,MADRS2))\n",
    "\n",
    "# discard NaN, keep the average of only condition subjects for convenience\n",
    "import math\n",
    "nanvals = np.where(np.isnan(MADRS))\n",
    "MADRScondition = np.delete(MADRS, nanvals[1], 1)\n",
    "MADRScondition = np.mean(MADRScondition, axis = 0)\n",
    "MADRScontrol = (np.ones(32))*(-1)\n",
    "\n",
    "print(MADRScondition)\n",
    "print(MADRScontrol)\n",
    "print(MADRScondition.shape)\n",
    "print(MADRScontrol.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_timestamps(timestamps): # reformatting time series data\n",
    "    from datetime import datetime\n",
    "    HH_MM = [0, 0]*len(timestamps)   \n",
    "    for i in range(len(timestamps)):\n",
    "        t = timestamps.iloc[i][0]\n",
    "        d = datetime.strptime(t, '%Y-%m-%d %H:%M:%S')       \n",
    "        h,m = d.hour, d.minute\n",
    "        HH_MM[i] = h,m        \n",
    "    return HH_MM\n",
    "\n",
    "def read_subject(cohort,subj): # read subject data from the folder\n",
    "    filename = f\"{cohort}_{subj}.csv\"\n",
    "    file_path = os.path.join(folder_path, cohort, filename)\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return None, None  # Return None if the file is missing\n",
    "    \n",
    "    try:\n",
    "        file = pd.read_csv(file_path, header=0)\n",
    "        timestamps = file.iloc[:, [0]]\n",
    "        activity = file.iloc[:, [2]]\n",
    "        \n",
    "        HH_MM = convert_timestamps(timestamps)\n",
    "\n",
    "        # Convert dataframe to array\n",
    "        activity = np.array(activity).flatten()\n",
    "\n",
    "        return HH_MM, activity\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {filename}: {e}\")\n",
    "        return None, None  # Handle errors gracefully\n",
    "\n",
    "def rearrange_in_days(HH_MM, activity): # stacks days of recorded activity\n",
    "    # Find indices where time is midnight (00:00)\n",
    "    midnights = idx(HH_MM,(0,0))    \n",
    "    # Extract each day's data\n",
    "    for i in range(0,len(midnights)-1):\n",
    "        day_i = activity[midnights[i]:(midnights[i+1]-1)]\n",
    "        if i==0: rec = day_i\n",
    "        elif len(day_i)==minutesinaday: rec = np.vstack((rec,day_i))\n",
    "    return rec\n",
    "\n",
    "def gather_data(cohort,subj): # bridge function: use rearrange_in_days and read_subject together\n",
    "    HH_MM,activity = read_subject(cohort,subj)\n",
    "    # Rearrange activity data by days\n",
    "    days = rearrange_in_days(HH_MM, activity) \n",
    "    return days\n",
    "\n",
    "def idx(l, target): # returns the position of every occurence of 'target'\n",
    "    output = []\n",
    "    for i in range(len(l)):\n",
    "        if l[i]==target:\n",
    "            output.append(i)\n",
    "    return output\n",
    "\n",
    "def get_subj_index(cohort,subj,I): # finding specific subjects' data in a structured array\n",
    "    if cohort == \"condition\":\n",
    "        subj = subj+32  \n",
    "    i = np.where(I == subj)\n",
    "    istart = i[0][0]\n",
    "    iend = i[0][-1]\n",
    "    return (istart, iend)\n",
    "\n",
    "def visualize_data(x,y,style,col = 'black',T ='', l='',xl='', yl='',xlimit=None):\n",
    "    import matplotlib.pyplot as plt\n",
    "    if style =='plot':\n",
    "        plt.plot(x,y,color = col, label = l)\n",
    "    if style =='scatter':\n",
    "        plt.scatter(x,y,color = col, label = l)  \n",
    "    plt.xlabel(xl)\n",
    "    plt.ylabel(yl)\n",
    "    plt.title(T)\n",
    "    if not(xlimit is None):\n",
    "        plt.xlim((xlimit[0],xlimit[1]))\n",
    "    return None  \n",
    "    \n",
    "def feature_extract(X):\n",
    "    featurevec = []\n",
    "    t = np.linspace(0,360-(360/1439),1439)\n",
    "    # Extract features for each day's data\n",
    "    for day in X:\n",
    "        f1 = np.mean(day)  # Mean activity\n",
    "        f2 = np.std(day)   # Standard deviation of activity\n",
    "        f3 = np.max(day)   # Maximum activity\n",
    "        f,f4,f5,f6= cosinor(t,day)  # Cosinor analysis features\n",
    "        f8 = np.sum(day)**2  # Squared sum of activity\n",
    "        featurevec.append([f1,f2,f3,f5,f6])\n",
    "    \n",
    "    X_feat = np.array(featurevec)\n",
    "    return X_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# initializing X and Y arrays\n",
    "Ncontr = 32 # number of control subjects\n",
    "Ncond  = 23 # number of condition subjects\n",
    "minutesinaday = 1439 # number of minutes from 00:00 to 23:59\n",
    "\n",
    "# Initialize counter for subject IDs\n",
    "counter = 1\n",
    "\n",
    "# Process control subjects\n",
    "for subj in range(1,Ncontr):\n",
    "    cohort = \"control\"\n",
    "    \n",
    "    # Get activity data for current subject\n",
    "    days = gather_data(cohort,subj)\n",
    "    ndays = days.shape[0]\n",
    "    \n",
    "    # Create arrays of zeros for MADRS scores and subject IDs\n",
    "    MADRSzeros = np.zeros(ndays,dtype = int)\n",
    "    subj_id= np.ones(ndays,dtype = int) * counter\n",
    "    counter += 1\n",
    "    \n",
    "    if subj == 1: # Initialize main arrays with first subject\n",
    "        Y = MADRSzeros # MADRS scores array\n",
    "        X = days       # Activity data array  \n",
    "        ID= subj_id    # Subject ID array\n",
    "        \n",
    "    else:         # Add subsequent subjects to arrays\n",
    "        X = np.vstack((X,days))        # Stack activity data vertically\n",
    "        Y = np.hstack((Y,MADRSzeros))  # Concatenate MADRS scores horizontally\n",
    "        ID= np.hstack((ID,subj_id))    # Concatenate subject IDs horizontally\n",
    "\n",
    "# Process condition subjects    \n",
    "for subj in range(1, Ncond):\n",
    "    cohort = \"condition\"\n",
    "    \n",
    "    days = gather_data(cohort,subj)\n",
    "    ndays = days.shape[0]\n",
    "    \n",
    "    # Create arrays of ones for MADRS scores and subject IDs\n",
    "    MADRSones = np.ones(ndays,dtype = int)\n",
    "    subj_id= np.ones(ndays,dtype = int) * counter\n",
    "    counter += 1\n",
    "    \n",
    "    # Add condition subjects to existing arrays\n",
    "    X = np.vstack((X,days))        # Stack activity data vertically\n",
    "    Y = np.hstack((Y,MADRSones))   # Concatenate MADRS scores horizontally\n",
    "    ID= np.hstack((ID,subj_id))    # Concatenate subject IDs horizontally\n",
    "    \n",
    "print(X)\n",
    "print(Y)\n",
    "print(ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create time axis array from 0 to 24 hours with 0.0167 hour intervals\n",
    "import matplotlib.pyplot as plt  \n",
    "ax = np.arange(0, 24+0.0167, 0.0167)\n",
    "\n",
    "# Create 2x2 subplot grid showing activity data for first 4 days\n",
    "plt.figure(0)\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(ax, X[0])\n",
    "plt.ylabel(\"activity [g/min]\"), plt.xlabel(\"0-24H\"),plt.xlim(0,24)\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(ax, X[1])\n",
    "plt.xlabel(\"0-24H\"),plt.xlim(0,24)\n",
    "plt.subplot(2,2,3)\n",
    "plt.plot(ax, X[2])\n",
    "plt.ylabel(\"activity [g/min]\"), plt.xlabel(\"0-24H\"),plt.xlim(0,24)\n",
    "plt.subplot(2,2,4)\n",
    "plt.plot(ax, X[3])\n",
    "plt.xlabel(\"0-24H\"),plt.xlim(0,24)\n",
    "plt.suptitle(\"CONTROL subj. 1, days 1 to 4\")\n",
    "\n",
    "# Identify days with suspiciously low activity (below threshold)\n",
    "threshold = 25 \n",
    "X_mean = np.mean(X,axis=1)  # Calculate mean activity for each day\n",
    "sus = np.where(X_mean<threshold)  # Find indices where mean is below threshold\n",
    "sus = sus[0]\n",
    "print(\"there are approximately \",len(sus),\" suspiciously low-activity days\")\n",
    "\n",
    "# Plot the suspicious days in a 1x4 subplot grid\n",
    "plt.figure()\n",
    "plt.subplot(1,4,1)\n",
    "plt.plot(ax,X[sus[0]]),plt.ylabel(\"activity [g/min]\"), plt.xlabel(\"0-24H\"),plt.xlim(0,24)\n",
    "plt.subplot(1,4,2)\n",
    "plt.plot(ax,X[sus[1]]), plt.xlabel(\"0-24H\"),plt.xlim(0,24)\n",
    "plt.subplot(1,4,3)\n",
    "plt.plot(ax,X[sus[2]]),plt.ylabel(\"activity [g/min]\"), plt.xlabel(\"0-24H\"),plt.xlim(0,24)\n",
    "plt.subplot(1,4,4)\n",
    "plt.plot(ax,X[sus[3]]), plt.xlabel(\"0-24H\"),plt.xlim(0,24)\n",
    "plt.suptitle(\"suspicious recordings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print original dataset dimensions\n",
    "print(\"original dataset size: \",X.shape)\n",
    "\n",
    "# Remove suspicious data points from X, Y and ID arrays\n",
    "X = np.delete(X,sus,axis=0)\n",
    "Y = np.delete(Y,sus,axis=0)\n",
    "ID= np.delete(ID,sus,axis=0)\n",
    "\n",
    "# Print dimensions after cleaning\n",
    "print(\"cleaned dataset size: \",X.shape)\n",
    "\n",
    "# Function to smooth time series data using moving average filter\n",
    "def smooth(day,win):\n",
    "    from scipy import signal\n",
    "    import numpy as np\n",
    "    \n",
    "    L = win # moving average filter window length\n",
    "    b = (np.ones(L))/L # Create filter coefficients (uniform weights)\n",
    "    a = np.ones(1)     # Denominator coefficient for filter\n",
    "    x = day            # Input signal\n",
    "    y = signal.lfilter(b,a,x) # Apply filter to signal\n",
    "    return y\n",
    "\n",
    "# Apply 20-minute moving average filter to each day's data\n",
    "window = 20\n",
    "for i in range(0,X.shape[0]):\n",
    "    day = X[i]\n",
    "    smoothed_day = smooth(day,window)\n",
    "    \n",
    "    # Initialize or stack smoothed data\n",
    "    if i==0: \n",
    "        X_smooth = smoothed_day\n",
    "    else: \n",
    "        X_smooth = np.vstack((X_smooth,smoothed_day))\n",
    "\n",
    "# Subplot figure showing smoothed activity data for first 4 days    \n",
    "plt.figure()\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(ax, X_smooth[0])\n",
    "plt.ylabel(\"activity [g/min]\"), plt.xlabel(\"0-24H\"),plt.xlim(0,24)\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(ax, X_smooth[1])\n",
    "plt.xlabel(\"0-24H\"),plt.xlim(0,24)\n",
    "plt.subplot(2,2,3)\n",
    "plt.plot(ax, X_smooth[2])\n",
    "plt.ylabel(\"activity [g/min]\"), plt.xlabel(\"0-24H\"),plt.xlim(0,24)\n",
    "plt.subplot(2,2,4)\n",
    "plt.plot(ax, X_smooth[3])\n",
    "plt.xlabel(\"0-24H\"),plt.xlim(0,24)\n",
    "plt.suptitle(\"smoothed CONTROL subj. 1, days 1 to 4\")\n",
    "\n",
    "# Comparison plot of raw vs smoothed data for first day\n",
    "plt.figure()\n",
    "plt.title(\"20-minutes moving average\")\n",
    "plt.plot(ax,X[0],color='r',label=\"raw\"),plt.ylabel(\"activity [g/min]\")\n",
    "plt.plot(ax,X_smooth[0],color='b',label=\"smoothed\"),plt.xlabel(\"0-24H\"),plt.xlim(0,24)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying data dimensions after cleaning\n",
    "#print(X.shape)\n",
    "#print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)  # Standardizing features\n",
    "\n",
    "# Polynomial features for non-linearity\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly.fit_transform(X_scaled)  # expanding features\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_poly, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Ridge Regression\n",
    "ridge = Ridge(alpha=1.0)  # Regularization strength\n",
    "ridge.fit(x_train, y_train)\n",
    "\n",
    "y_prediction = ridge.predict(x_test)\n",
    "\n",
    "r2 = r2_score(y_test, y_prediction)\n",
    "mse = mean_squared_error(y_test, y_prediction)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(\"Results for Linear Regression model\")\n",
    "print(\"r2 score is \" + str(r2))\n",
    "print(\"mean_sqrd_error is \"+ str(mse))\n",
    "print(\"root_mean_squared error is \" + str(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)  \n",
    "\n",
    "# Stratified train-test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_scaled, Y, test_size=0.2, random_state=42, stratify=Y)\n",
    "\n",
    "# optimizing\n",
    "XGB = XGBClassifier(\n",
    "    n_estimators=200,       # More trees\n",
    "    learning_rate=0.05,     # Slower learning rate \n",
    "    max_depth=5,            \n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 5-fold cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = cross_val_score(XGB, X_scaled, Y, cv=cv, scoring='accuracy')\n",
    "print(f\"Cross-Validation Accuracy: {np.mean(cv_scores):.4f} ± {np.std(cv_scores):.4f}\")\n",
    "\n",
    "XGB.fit(x_train, y_train)\n",
    "\n",
    "y_pred = XGB.predict(x_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"XGBoost Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Support Vector Machine (SVM)\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_scaled, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Hyperparameter tuning with grid search\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "svm_model = SVC(probability=True, random_state=42)\n",
    "grid_search = GridSearchCV(svm_model, param_grid, cv=5, scoring='accuracy', verbose=2, n_jobs=-1)\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "best_svm = grid_search.best_estimator_\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "\n",
    "y_pred = best_svm.predict(x_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"SVM Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Regressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_scaled, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# optimizing hyperparameters\n",
    "RF = RandomForestRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=10,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=3,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# 5-fold cross-validation\n",
    "cv_scores = cross_val_score(RF, X_scaled, Y, cv=5, scoring='r2')\n",
    "\n",
    "print(f\"Cross-Validation R² Score: {np.mean(cv_scores):.4f} ± {np.std(cv_scores):.4f}\")\n",
    "\n",
    "RF.fit(x_train, y_train)\n",
    "\n",
    "y_pred = RF.predict(x_test)\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"Test R² Score: {r2:.4f}\")\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Layer Perceptron (manual tuning)\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_scaled, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "MLP = MLPClassifier(\n",
    "    hidden_layer_sizes=(64, 32, 16),  # deeper network\n",
    "    activation='relu',\n",
    "    solver='adam',                     # gradient descent\n",
    "    alpha=0.0001,\n",
    "    batch_size=64,\n",
    "    learning_rate_init=0.005,          # Lower LR\n",
    "    max_iter=2000,\n",
    "    early_stopping=True,\n",
    "    n_iter_no_change=20,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "MLP.fit(x_train, y_train)\n",
    "\n",
    "y_pred = MLP.predict(x_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"MLP Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multi-Layer Perceptron (MLP) with grid-search optimization\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_scaled, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Defining MLP with optimized hyperparameters\n",
    "MLP = MLPClassifier(max_iter=1000, early_stopping=True, random_state=42)\n",
    "\n",
    "# Hyperparameter tuning with GridSearch\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(512, 256, 128), (256, 128, 64)], #different NN architectures\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'solver': ['lbfgs', 'adam'],\n",
    "    'alpha': [0.0001, 0.001], #L2 regularizatino parameter\n",
    "    'learning_rate_init': [0.001, 0.0005],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(MLP, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=2)\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "# Best model from GridSearch\n",
    "best_mlp = grid_search.best_estimator_\n",
    "\n",
    "y_pred = best_mlp.predict(x_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Optimized MLP Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
